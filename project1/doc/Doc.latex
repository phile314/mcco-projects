\documentclass[12pt, a4paper, twoside]{article}
\usepackage{hyperref}
% font size could be 10pt (default), 11pt or 12 pt
% paper size coulde be letterpaper (default), legalpaper, executivepaper,
% a4paper, a5paper or b5paper
% side coulde be oneside (default) or twoside 
% columns coulde be onecolumn (default) or twocolumn
% graphics coulde be final (default) or draft 
%
% titlepage coulde be notitlepage (default) or titlepage which 
% makes an extra page for title 
% 
% paper alignment coulde be portrait (default) or landscape 
%
% equations coulde be 
%   default number of the equation on the rigth and equation centered 
%   leqno number on the left and equation centered 
%   fleqn number on the rigth and  equation on the left side
%   
\title{MMCO Project A}
\author{Marco Vassena  \\
    4110161 \\
    \and 
    Philipp Hausmann \\
    4003373 \\
    }

\date{\today} 
\begin{document}

\maketitle
\section{Features}

\subsection{Implemented}
\begin{itemize}
    \item Basic escape codes
    \item Correct validation and printing of required, optional and "and/or" fields.
    \item Special Characters, eg "-\--" becomes "--" and support for \{\textbackslash"o\} style escapes
    \item Different rendering for different fields, e.g. the editor field is prefixed with "In:" and followed by ", editors". TODO
    \item what else?
\end{itemize}

\subsection{Not Implemented}
\begin{itemize}
    \item Lexing/Parsing values enclosed by brackets, "\{....\}".
    \item No warning for duplicate bibtex entry keys
    \item The user-visible key for entry indexing (eg "[LO98]") could be non-unique if the author name and year are similar.
\end{itemize}

TODO we should say why we haven't implement the brackts stuff
this also goes for other (important) unimplemented features

\section{Lexing}
The lexer produces a stream of \texttt{Token} that are later parsed by the \texttt{TokenParser}. 
The tokens produced are: commas, brackets, equal signs, identifiers and values. Identifiers denotes entry types, fields names and keys and are lexed as a non-null sequence of alphanumeric characters starting with a letter. They could actually be extended to support some few other special symbols.
Values represent the right hand side of a field declaration.
They are either a number or a string in quotes.
During the lexing phase white space and comments are disregarded. In addition latex special syntax (e.g. \{\textbackslash''o\}) is detected and directly mapped to the proper unicode character. The list of supported escaping rules is not complete and those unsupported rules
will be left unchanged.
As a final remark the bracket syntax for specifying values within a bibtex entry is not supported. In order to provide this feature the lexer should be context sensitive, otherwise it would be impossible to disambiguate a value from a whole bibtex entry. Since the \texttt{CCO} library, that we were required to use, does not provide context sensitive lexing, we could not implement this feature. A possible workaround would be to tokenize the right hand side and then recompose the value during the parsing phase converting back the tokens between brackets to strings.  Although this could be possible with a context free lexer, we decided not to follow this approach because it is muddled and unnatural.

\section{Bibtex Entry Validation}
A bibtex entry basically consists of a (hopefully) unique key, a type and
a list of fields and their values.

For each type of bibtex entry, a different set of fields might be required or
optionally given. Furthermore, some of this fields might be mutually exclusive/conflicting,
meaning that only one of them should be given. In the end, the following tasks
need to be carried out:
\begin{enumerate}
    \item Generate error messages for missing fields
    \item Generate warning messages for conflicting fields, resolve conflict by dropping a field
    \item Order the fields in their printing order
    \item Drop unknown fields
\end{enumerate}

To encode all this information in a concise manner, a tree-like data structure seems to be the
best approach. Trees representing the bibtex specification are built where the nodes
indicate the ways how to combine their children, eg. to concatenate them, to choose
one of the children, and so on.

The order in this tree can be used to perform the sorting, and all fields not present
in the tree are dropped.

\subsection{Implementation}
Building the tree specification is a rather straight forward one-to-one translation
of the bibtex specification found at (http://www.andy-roberts.net/res/writing/latex/bibentries.pdf).

More involved is using the given tree. Basically, all tasks are carried out
as tree folds/walks. The UUAGC system was chosen to implement this as it allows
the expression of tree folds in a convenient way.

All folds carry all field names and values around as input. The output is
the modified list of fields/values suitable for printing. Additionally,
warning and error messages are returned as well.

From the outside, either at least one error message is emitted through the
Feedback monad or a modified BibtexEntry with the new fields and values.


\section{Input/Output Encoding}
\subsection{Bibtex Input}
The bibtex input file should only contain valid ascii characters. Input
containing any other characters might lead to wrong output, altough
this has not yet been tested.

\subsection{Intermediate Results}
All programs assume that unicode characters in an ATerm value
are (de-)serialized correctly by the CCO library. As this
is not the case, Unicode support is broken. To work around
this problem, the -\--sane option can be passed to all programs.
This option causes the use of the prelude show/read functions
instead of the cco printer/parser functions.

As far as one can tell, the cause are non-matching escape/unescape mechanisms.

The pretty printer in file src/CCO/Tree/ATerm.hs:93 calls the prelude show
function on the string to print. According to the source code of show,
most unicode characters are encoded using \textbackslash and the base 10 representation of the unicode codepoint.
 (See \url{http://hackage.haskell.org/package/base-4.6.0.1/docs/src/GHC-Show.html#showLitChar})

The cco lexer (src/CCO/Tree/ATerm/Lexer.hs) on the other hand reads escape codes of this format. The two
errors it makes are:
\begin{enumerate}
    \item It accepts only escape codes with exeactly 3 digits. Some unicode code points need more than 3 digits to represent.
    \item It assumes that the digits are in base 8, but they are in base 10 as can be seen in the show source code.
\end{enumerate}

\subsection{Html Output}
The pp-html program assumes that unicode strings are correctly printed by the cco component library. This seems to be the case on a Linux Fedora 19
installation. It is assumed that the output is stored in utf-8, as this is the most popular and sensible encoding for html documents nowadays.

Please note that the cco library by default does not produce a unicode byte order mark (BOM, \url{http://en.wikipedia.org/wiki/Byte_order_mark}).
A byte order mark is not mandatory for utf-8 encoded files, but at least firefox uses the wrong encoding if no byte order mark is
present when opening files locally with the "file://" scheme. See for example \url{https://bugzilla.mozilla.org/show_bug.cgi?id=760050}.
As a workaround the encoding can be corrected manually in most browsers in the menu.

As html documents are normally intended to be served over the http(s) protocol, this behaviour is not considered to be a bug in our program.
\end{document}
